####################
##                ##
## LABORATORY LOG ##
##                ##
####################

#############################
# EXPEDITION 1: 20 AUG 2018 #
#############################

QUINTESSENCE FROM:

1998 Yan LeCun et al Efficient Backprop 

MOTIVATION: To improve the process of finding error minimum

### SHORT REPORT:

# 1 # Shuffle the examples
# 2 # Center the input variables by subtracting the means
# 3 # Normalize the input variable to a standard deviation of 1
# 4 # If possible, decorrelate the input variables
# 5 # Pick a network with a sigmoid function
# 6 # Set the target values within the range of the sigmoid, +-1
# 7 # Initialize the weights to random values

### LONG REPORT:

#####
# 1 # The choice ANN model, architecture and cost function
##### is crucial for obtaining a network tha generalises well.
#     If a wrong model is chosen and no proper architecture is
#     is selected then even superb minimization will not help.
################################################################

#####
# 2 # Batch Learning: At each iteration we pass through all the 
##### dataset in order to compute the average or true gradient.
#     All data must be considered before weights are updated.
#
#     Stochastic (online) Learning: a single example is chosen
#     (e.g.) randomly from the training set at each iteration. 
#     The noise produced by such a process aids in the        
#     generalisation capabilities.
#
#     Advantages of Stochastic Learning:
#     1) Much faster than batch Learning
#     2) Often results in better solutions
#     3) Can be used for tracking changes
#     4) Super effective against datasets with duplicate values
#
#     Advantages of Batch Learning:
#     1) Conditions of convergence are well understood
#     2) Many acceleration techniques only operate in batch 
#     learning
#     3) Theoretical analysis of the weight dynamics and 
#     convergence rates are simpler.
################################################################

#####
# 3 # Shuffle the training set so that successive training 
##### examples never belong to the same class.
#     
#     Present input examples that produce a large error more
#     frequently than examples that produce a small error. Not 
#     applicable to outliers.
################################################################

#####
# 4 # Normalizing the Inputs. Any shift of the average input
##### away from zero will bias the update in a particular
#     direction and thus slow down learning. Therefore, it is
#     good to shift the input so that the average over the
#     training set is close to zero. This heuristic should be
#     applied at all layers, which means that we want the 
#     average of the output of a node to be close to zero
#     because these outputs are the inouts to the next layer.
#
#     Scale the inputs so that they have the same covariance
#     as well. Excepition being, insignificant inputs.
#
#     Input variables should be uncorrelated if possible. PCA
#     can be used to decorrelate the inputs. Inputs that are 
#     linearly dependent may also produce degeneracies which 
#     may also slow learning.
################################################################

#####
# 5 # Tanh is often better than logistic. See paper for a
##### suggestion. 
################################################################

#####
# 6 # Weights should be chosen randomly (Uniform Distribution)
################################################################

#####
# 7 # Learning rate. Give each weight its own learning rate. 
##### Learning rates should be proportional to the square root
#     of the number of inputs to the unit. Weights in lower
#     layers should typically be larger than in the higher
#     layers.
#
#     Momentum generally helps more in batch mode than in
#     stochastic mode, but no systematic study of this are
#     known to the authors.

#############################
# EXPEDITION 2: 21 AUG 2018 #
#############################

QUINTESSENCE FROM:

2011 Glorot et al Deep Sparse Rectifier Neural Networks 

MOTIVATION: To show that rectifying neurons are an
            even better model of biological neurons and
            yield equal or better performance than hyperbolic
            tangent networks in spite of the
            hard non-linearity and non-dierentiability
            at zero, creating sparse representations with
            true zeros, which seem remarkably suitable
            for naturally sparse data.


### LONG REPORT

#####
# 1 # something that can be considered a breakthrough happened
##### in 2006, with the introduction of Deep Belief Networks
#     (Hinton et al., 2006), and more generally the
#     idea of initializing each layer by unsupervised learning
#     (Bengio et al., 2007; Ranzato et al., 2007).
################################################################

#####
# 2 # Neuroscience Observations
##### For models of biological neurons, the activation
#     function is the expected ring rate as a function of the
#     total input currently arising out of incoming signals
#     at synapses (Dayan and Abott, 2001). An activation
#     function is termed, respectively antisymmetric or sym-
#     metric when its response to the opposite of a strongly
#     excitatory input pattern is respectively a strongly 
#     inhibitory or excitatory one, and one-sided when this
#     response is zero.
################################################################

#####
# 3 #
#####

The main gaps that we wish to consider
between computational neuroscience models and
machine learning models are the following:

A) Studies on brain energy expense suggest that
neurons encode information in a sparse and distributed
way (Attwell and Laughlin, 2001), estimating
the percentage of neurons active at the
same time to be between 1 and 4% (Lennie, 2003).
This corresponds to a trade-o between richness
of representation and small action potential energy
expenditure. Without additional regularization,
such as an L1 penalty, ordinary feedforward
neural nets do not have this property. For example,
the sigmoid activation has a steady state
regime around 1
2 , therefore, after initializing with
small weights, all neurons re at half their saturation
regime. This is biologically implausible and
hurts gradient-based optimization (LeCun et al.,
1998; Bengio and Glorot, 2010).

B) Important divergences between biological and
machine learning models concern non-linear
activation functions. A common biological
model of neuron, the leaky integrate-and-re (or
LIF) (Dayan and Abott, 2001), gives the following
relation between the ring rate and the input
current (SEE PAPER)

#####
# 4 #
#####

Sparsity has become a concept of interest, not only in
computational neuroscience and machine learning but
also in statistics and signal processing (Candes and
Tao, 2005). We show here that
using a rectifying non-linearity gives rise to real zeros
of activations and thus truly sparse representations. From a computational point of view, such representations
are appealing for the following reasons:

A) Information disentangling. One of the
claimed objectives of deep learning algorithms
(Bengio, 2009) is to disentangle the
factors explaining the variations in the data. A
dense representation is highly entangled because
almost any change in the input modies most of the entries in the representation vector. Instead,
if a representation is both sparse and robust to
small input changes, the set of non-zero features
is almost always roughly conserved by small
changes of the input.

B) Effcient variable-size representation. Different
inputs may contain different amounts of information
and would be more conveniently represented
using a variable-size data-structure, which
is common in computer representations of information.
Varying the number of active neurons
allows a model to control the effective dimension-
ality of the representation for a given input and
the required precision.

C) Linear separability. Sparse representations are
also more likely to be linearly separable, or more
easily separable with less non-linear machinery,
simply because the information is represented in
a high-dimensional space. Besides, this can re
ect
the original data format. In text-related applications
for instance, the original raw data is already
very sparse (see Section 4.2).

D) Distributed but sparse. Dense distributed representations
are the richest representations, being
potentially exponentially more ecient than
purely local ones (Bengio, 2009). Sparse representations'
eciency is still exponentially greater,
with the power of the exponent being the number
of non-zero features. They may represent a good
trade-o with respect to the above criteria.

Nevertheless, forcing too much sparsity may hurt predictive
performance for an equal number of neurons,
because it reduces the eective capacity of the model.

#####
# 5 #
#####

Rectifier Neurons.
The neuroscience literature (Bush and Sejnowski,
1995; Douglas and al., 2003) indicates that corti-
cal neurons are rarely in their maximum saturation
regime, and suggests that their activation function can
be approximated by a rectier. Most previous studies
of neural networks involving a rectifying activation
function concern recurrent networks (Salinas and Abbott,
1996; Hahnloser, 1998).
The rectier function rectier(x) = max(0; x) is onesided
and therefore does not enforce a sign symmetry1
or antisymmetry1: instead, the response to the opposite
of an excitatory input pattern is 0 (no response).
However, we can obtain symmetry or antisymmetry by
combining two rectier units sharing parameters.

A) Advantages The rectier activation function allows
a network to easily obtain sparse representations. For
example, after uniform initialization of the weights,
around 50% of hidden units continuous output values
are real zeros, and this fraction can easily increase
with sparsity-inducing regularization. Apart from being
more biologically plausible, sparsity also leads to
mathematical advantages (see previous section).

B) Potential Problems One may hypothesize that the
hard saturation at 0 may hurt optimization by blocking
gradient back-propagation. To evaluate the potential
impact of this eect we also investigate the softplus
activation: softplus(x) = log(1+ex) (Dugas et al.,
2001), a smooth version of the rectifying non-linearity.
We lose the exact sparsity, but may hope to gain easier
training. However, experimental results (see Section
4.1) tend to contradict that hypothesis, suggesting
that hard zeros can actually help supervised training.
We hypothesize that the hard non-linearities do not
hurt so long as the gradient can propagate along some
paths, i.e., that some of the hidden units in each layer
are non-zero. With the credit and blame assigned to
these ON units rather than distributed more evenly, we
hypothesize that optimization is easier. Another problem
could arise due to the unbounded behavior of the
activations; one may thus want to use a regularizer to
prevent potential numerical problems. Therefore, we
use the L1 penalty on the activation values, which also
promotes additional sparsity. Also recall that, in order
to eciently represent symmetric/antisymmetric
behavior in the data, a rectier network would need twice as many hidden units as a network of symmetric/
antisymmetric activation functions.


#############################
# EXPEDITION 3: 23 AUG 2018 #
#############################

QUINTESSENCE FROM:

The Deep Learning Book

MOTIVATION: An 800-page comprehensive book on deep learning theory

### SHORT REPORT:

### LONG REPORT:

#####
# 1 #
#####

Constructing matrices with specific eigenvalues and eigenvectors
allows us to stretch space in desired directions.

#####
# 2 #
#####

Instead of using a single function to represent a probability distribution, we
can split a probability distribution into many factors that we multiply together.
For example, suppose we have three random variables: a, b and c. Suppose that
a influences the value of b and b influences the value of c, but that a and c are
independent given b. We can represent the probability distribution over all three
variables as a product of probability distributions over two variables:
p(a, b, c) = p(a)p(b | a)p(c | b).

#####
# 3 #
#####

We can
thus reduce f (x) by moving x in small steps with opposite sign of the derivative.
This technique is called gradient descent

#####
# 4 #
#####

When f (x) = 0, the derivative provides no information about which direction
to move. Points where f(x) = 0 are known as critical points or stationary
points. A local minimum is a point where f (x) is lower than at all neighboring
points, so it is no longer possible to decrease f(x) by making infinitesimal steps.
A local maximum is a point where f (x) is higher than at all neighboring points,

#####
# 5 #
#####

The second derivative tells
us how the first derivative will change as we vary the input. This is important
because it tells us whether a gradient step will cause as much of an improvement
as we would expect based on the gradient alone. We can think of the second
derivative as measuring curvature.
If such a function has a second derivative of zero,
then there is no curvature. It is a perfectly flat line, and its value can be predicted
using only the gradient. If the gradient is 1, then we can make a step of size 
along the negative gradient, and the cost function will decrease by . If the second
derivative is negative, the function curves downward, so the cost function will
actually decrease by more than . Finally, if the second derivative is positive, the
function curves upward, so the cost function can decrease by less than .

#####
# 6 #
#####

When our function has multiple input dimensions, there are many second
derivatives. These derivatives can be collected together into a matrix called the
Hessian matrix.
The second derivative in a specific direction represented by a unit
vector d is given by dHd. When d is an eigenvector of H , the second derivative
in that direction is given by the corresponding eigenvalue. For other directions of
d, the directional second derivative is a weighted average of all of the eigenvalues,
with weights between 0 and 1, and eigenvectors that have smaller angle with d
receiving more weight. The maximum eigenvalue determines the maximum second
derivative and the minimum eigenvalue determines the minimum second derivative.

#####
# 7 #
#####

The second derivative can be used to determine whether a critical point is
a local maximum, a local minimum, or saddle point. Recall that on a critical
point, f (x) = 0. When the second derivative f(x) > 0, the first derivative f (x)
increases as we move to the right and decreases as we move to the left. This means f (x − ) < 0 and f (x + ) > 0 for small enough . In other words, as we move
right, the slope begins to point uphill to the right, and as we move left, the slope
begins to point uphill to the left. Thus, when f (x) = 0 and f(x) > 0, we can
conclude that x is a local minimum. Similarly, when f (x) = 0 and f (x) < 0, we
can conclude that x is a local maximum. This is known as the second derivative
test. Unfortunately, when f (x) = 0, the test is inconclusive. In this case x may
be a saddle point, or a part of a flat region.

#####
# 8 #
#####

In multiple dimensions, we need to examine all of the second derivatives of the
function. Using the eigendecomposition of the Hessian matrix, we can generalize
the second derivative test to multiple dimensions. At a critical point, where
∇xf (x) = 0, we can examine the eigenvalues of the Hessian to determine whether
the critical point is a local maximum, local minimum, or saddle point. When the
Hessian is positive definite (all its eigenvalues are positive), the point is a local
minimum. This can be seen by observing that the directional second derivative
in any direction must be positive, and making reference to the univariate second
derivative test. Likewise, when the Hessian is negative definite (all its eigenvalues
are negative), the point is a local maximum. In multiple dimensions, it is actually
possible to find positive evidence of saddle points in some cases. When at least
one eigenvalue is positive and at least one eigenvalue is negative, we know that
x is a local maximum on one cross section of f but a local minimum on another
cross section. See figure 4.5 for an example. Finally, the multidimensional second
derivative test can be inconclusive, just like the univariate version. The test is
inconclusive whenever all of the non-zero eigenvalues have the same sign, but at
least one eigenvalue is zero. This is because the univariate second derivative test is
inconclusive in the cross section corresponding to the zero eigenvalue.

#####
# 9 #
#####

Optimization algorithms that use only the gradient, such as gradient descent,
are called first-order optimization algorithms. Optimization algorithms that
also use the Hessian matrix, such as Newton’s method, are called second-order
optimization algorithms

######
# 10 #
######

How can we affect performance on the test set when we get to observe only the training set? The field of statistical learning theory provides some answers. If
the training and the test set are collected arbitrarily, there is indeed little we can
do. If we are allowed to make some assumptions about how the training and test
set are collected, then we can make some progress.
The train and test data are generated by a probability distribution over datasets
called the data generating process. We typically make a set of assumptions
known collectively as the i.i.d. assumptions. These assumptions are that the
examples in each dataset are independent from each other, and that the train
set and test set are identically distributed, drawn from the same probability
distribution as each other. This assumption allows us to describe the data generating
process with a probability distribution over a single example. The same
distribution is then used to generate every train example and every test example.
We call that shared underlying distribution the data generating distribution,
denoted pdata. This probabilistic framework and the i.i.d. assumptions allow us to
mathematically study the relationship between training error and test error.

######
# 11 #
######

In our weight decay example, we expressed our preference for linear functions
defined with smaller weights explicitly, via an extra term in the criterion we
minimize. There are many other ways of expressing preferences for different
solutions, both implicitly and explicitly. Together, these different approaches
are known as regularization. Regularization is any modification we make to a
learning algorithm that is intended to reduce its generalization error but not its
training error. Regularization is one of the central concerns of the field of machine
learning, rivaled in its importance only by optimization.
The no free lunch theorem has made it clear that there is no best machine
learning algorithm, and, in particular, no best form of regularization. Instead
we must choose a form of regularization that is well-suited to the particular task
we want to solve. The philosophy of deep learning in general and this book in
particular is that a very wide range of tasks (such as all of the intellectual tasks
that people can do) may

######
# 12 #
######

Most machine learning algorithms have several settings that we can use to control
the behavior of the learning algorithm. These settings are called hyperparameters.
The values of hyperparameters are not adapted by the learning algorithm
itself (though we can design a nested learning procedure where one learning
algorithm learns the best hyperparameters for another learning algorithm).

######
# 13 #
######

When the dataset has hundreds of thousands of examples or more, this is not a
serious issue. When the dataset is too small, are alternative procedures enable one
to use all of the examples in the estimation of the mean test error, at the price of
increased computational cost. These procedures are based on the idea of repeating
the training and testing computation on different randomly chosen subsets or splits
of the original dataset. The most common of these is the k-fold cross-validation
procedure, shown in algorithm 5.1, in which a partition of the dataset is formed by
splitting it into k non-overlapping subsets. The test error may then be estimated
by taking the average test error across k trials.

######
# 14 #
######

The design of hidden units is an extremely active area of research and does not
yet have many definitive guiding theoretical principles.
Rectified linear units are an excellent default choice of hidden unit. Many other
types of hidden units are available. It can be difficult to determine when to use
which kind (though rectified linear units are usually an acceptable choice).

######
# 15 #
######

When a sigmoidal activation function must be used, the hyperbolic tangent
activation function typically performs better than the logistic sigmoid


######
# 16 #
######

Many strategies
used in machine learning are explicitly designed to reduce the test error, possibly
at the expense of increased training error. These strategies are known collectively
as regularization.