####################
##                ##
## LABORATORY LOG ##
##                ##
####################

#############################
# EXPEDITION 1: 20 AUG 2018 #
#############################

QUINTESSENCE FROM:

1998 Yan LeCun et al Efficient Backprop 

MOTIVATION: To improve the process of finding error minimum

### SHORT REPORT:

# 1 # Shuffle the examples
# 2 # Center the input variables by subtracting the means
# 3 # Normalize the input variable to a standard deviation of 1
# 4 # If possible, decorrelate the input variables
# 5 # Pick a network with a sigmoid function
# 6 # Set the target values within the range of the sigmoid, +-1
# 7 # Initialize the weights to random values

### LONG REPORT:

#####
# 1 # The choice ANN model, architecture and cost function
##### is crucial for obtaining a network tha generalises well.
#     If a wrong model is chosen and no proper architecture is
#     is selected then even superb minimization will not help.
################################################################

#####
# 2 # Batch Learning: At each iteration we pass through all the 
##### dataset in order to compute the average or true gradient.
#     All data must be considered before weights are updated.
#
#     Stochastic (online) Learning: a single example is chosen
#     (e.g.) randomly from the training set at each iteration. 
#     The noise produced by such a process aids in the        
#     generalisation capabilities.
#
#     Advantages of Stochastic Learning:
#     1) Much faster than batch Learning
#     2) Often results in better solutions
#     3) Can be used for tracking changes
#     4) Super effective against datasets with duplicate values
#
#     Advantages of Batch Learning:
#     1) Conditions of convergence are well understood
#     2) Many acceleration techniques only operate in batch 
#     learning
#     3) Theoretical analysis of the weight dynamics and 
#     convergence rates are simpler.
################################################################

#####
# 3 # Shuffle the training set so that successive training 
##### examples never belong to the same class.
#     
#     Present input examples that produce a large error more
#     frequently than examples that produce a small error. Not 
#     applicable to outliers.
################################################################

#####
# 4 # Normalizing the Inputs. Any shift of the average input
##### away from zero will bias the update in a particular
#     direction and thus slow down learning. Therefore, it is
#     good to shift the input so that the average over the
#     training set is close to zero. This heuristic should be
#     applied at all layers, which means that we want the 
#     average of the output of a node to be close to zero
#     because these outputs are the inouts to the next layer.
#
#     Scale the inputs so that they have the same covariance
#     as well. Excepition being, insignificant inputs.
#
#     Input variables should be uncorrelated if possible. PCA
#     can be used to decorrelate the inputs. Inputs that are 
#     linearly dependent may also produce degeneracies which 
#     may also slow learning.
################################################################

#####
# 5 # Tanh is often better than logistic. See paper for a
##### suggestion. 
################################################################

#####
# 6 # Weights should be chosen randomly (Uniform Distribution)
################################################################

#####
# 7 # Learning rate. Give each weight its own learning rate. 
##### Learning rates should be proportional to the square root
#     of the number of inputs to the unit. Weights in lower
#     layers should typically be larger than in the higher
#     layers.
#
#     Momentum generally helps more in batch mode than in
#     stochastic mode, but no systematic study of this are
#     known to the authors.

#############################
# EXPEDITION 2: 21 AUG 2018 #
#############################

QUINTESSENCE FROM:

2011 Glorot et al Deep Sparse Rectifier Neural Networks 

MOTIVATION: To show that rectifying neurons are an
            even better model of biological neurons and
            yield equal or better performance than hyperbolic
            tangent networks in spite of the
            hard non-linearity and non-dierentiability
            at zero, creating sparse representations with
            true zeros, which seem remarkably suitable
            for naturally sparse data.


### LONG REPORT

#####
# 1 # something that can be considered a breakthrough happened
##### in 2006, with the introduction of Deep Belief Networks
#     (Hinton et al., 2006), and more generally the
#     idea of initializing each layer by unsupervised learning
#     (Bengio et al., 2007; Ranzato et al., 2007).
################################################################

#####
# 2 # Neuroscience Observations
##### For models of biological neurons, the activation
#     function is the expected ring rate as a function of the
#     total input currently arising out of incoming signals
#     at synapses (Dayan and Abott, 2001). An activation
#     function is termed, respectively antisymmetric or sym-
#     metric when its response to the opposite of a strongly
#     excitatory input pattern is respectively a strongly 
#     inhibitory or excitatory one, and one-sided when this
#     response is zero.
################################################################

#####
# 3 #
#####

The main gaps that we wish to consider
between computational neuroscience models and
machine learning models are the following:

A) Studies on brain energy expense suggest that
neurons encode information in a sparse and distributed
way (Attwell and Laughlin, 2001), estimating
the percentage of neurons active at the
same time to be between 1 and 4% (Lennie, 2003).
This corresponds to a trade-o between richness
of representation and small action potential energy
expenditure. Without additional regularization,
such as an L1 penalty, ordinary feedforward
neural nets do not have this property. For example,
the sigmoid activation has a steady state
regime around 1
2 , therefore, after initializing with
small weights, all neurons re at half their saturation
regime. This is biologically implausible and
hurts gradient-based optimization (LeCun et al.,
1998; Bengio and Glorot, 2010).

B) Important divergences between biological and
machine learning models concern non-linear
activation functions. A common biological
model of neuron, the leaky integrate-and-re (or
LIF) (Dayan and Abott, 2001), gives the following
relation between the ring rate and the input
current (SEE PAPER)

#####
# 4 #
#####

Sparsity has become a concept of interest, not only in
computational neuroscience and machine learning but
also in statistics and signal processing (Candes and
Tao, 2005). We show here that
using a rectifying non-linearity gives rise to real zeros
of activations and thus truly sparse representations. From a computational point of view, such representations
are appealing for the following reasons:

A) Information disentangling. One of the
claimed objectives of deep learning algorithms
(Bengio, 2009) is to disentangle the
factors explaining the variations in the data. A
dense representation is highly entangled because
almost any change in the input modies most of the entries in the representation vector. Instead,
if a representation is both sparse and robust to
small input changes, the set of non-zero features
is almost always roughly conserved by small
changes of the input.

B) Effcient variable-size representation. Different
inputs may contain different amounts of information
and would be more conveniently represented
using a variable-size data-structure, which
is common in computer representations of information.
Varying the number of active neurons
allows a model to control the effective dimension-
ality of the representation for a given input and
the required precision.

C) Linear separability. Sparse representations are
also more likely to be linearly separable, or more
easily separable with less non-linear machinery,
simply because the information is represented in
a high-dimensional space. Besides, this can re
ect
the original data format. In text-related applications
for instance, the original raw data is already
very sparse (see Section 4.2).

D) Distributed but sparse. Dense distributed representations
are the richest representations, being
potentially exponentially more ecient than
purely local ones (Bengio, 2009). Sparse representations'
eciency is still exponentially greater,
with the power of the exponent being the number
of non-zero features. They may represent a good
trade-o with respect to the above criteria.

Nevertheless, forcing too much sparsity may hurt predictive
performance for an equal number of neurons,
because it reduces the eective capacity of the model.

#####
# 5 #
#####

Rectifier Neurons.
The neuroscience literature (Bush and Sejnowski,
1995; Douglas and al., 2003) indicates that corti-
cal neurons are rarely in their maximum saturation
regime, and suggests that their activation function can
be approximated by a rectier. Most previous studies
of neural networks involving a rectifying activation
function concern recurrent networks (Salinas and Abbott,
1996; Hahnloser, 1998).
The rectier function rectier(x) = max(0; x) is onesided
and therefore does not enforce a sign symmetry1
or antisymmetry1: instead, the response to the opposite
of an excitatory input pattern is 0 (no response).
However, we can obtain symmetry or antisymmetry by
combining two rectier units sharing parameters.

A) Advantages The rectier activation function allows
a network to easily obtain sparse representations. For
example, after uniform initialization of the weights,
around 50% of hidden units continuous output values
are real zeros, and this fraction can easily increase
with sparsity-inducing regularization. Apart from being
more biologically plausible, sparsity also leads to
mathematical advantages (see previous section).

B) Potential Problems One may hypothesize that the
hard saturation at 0 may hurt optimization by blocking
gradient back-propagation. To evaluate the potential
impact of this eect we also investigate the softplus
activation: softplus(x) = log(1+ex) (Dugas et al.,
2001), a smooth version of the rectifying non-linearity.
We lose the exact sparsity, but may hope to gain easier
training. However, experimental results (see Section
4.1) tend to contradict that hypothesis, suggesting
that hard zeros can actually help supervised training.
We hypothesize that the hard non-linearities do not
hurt so long as the gradient can propagate along some
paths, i.e., that some of the hidden units in each layer
are non-zero. With the credit and blame assigned to
these ON units rather than distributed more evenly, we
hypothesize that optimization is easier. Another problem
could arise due to the unbounded behavior of the
activations; one may thus want to use a regularizer to
prevent potential numerical problems. Therefore, we
use the L1 penalty on the activation values, which also
promotes additional sparsity. Also recall that, in order
to eciently represent symmetric/antisymmetric
behavior in the data, a rectier network would need twice as many hidden units as a network of symmetric/
antisymmetric activation functions.


#############################
# EXPEDITION 3: 23 AUG 2018 #
#############################

QUINTESSENCE FROM:

The Deep Learning Book

MOTIVATION: An 800-page comprehensive book on deep learning theory

### SHORT REPORT:

#####
# 1 #
#####

Constructing matrices with specific eigenvalues and eigenvectors
allows us to stretch space in desired directions.

#####
# 2 #
#####

Instead of using a single function to represent a probability distribution, we
can split a probability distribution into many factors that we multiply together.
For example, suppose we have three random variables: a, b and c. Suppose that
a influences the value of b and b influences the value of c, but that a and c are
independent given b. We can represent the probability distribution over all three
variables as a product of probability distributions over two variables:
p(a, b, c) = p(a)p(b | a)p(c | b).